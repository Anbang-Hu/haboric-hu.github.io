<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Fully Convolutional Networks in Halide (Checkpoint) | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Anbang Hu, Xingda Zhai
View us on Github
For project background information, please refer to our proposal webpage.
&amp;quot;Progress for progess&apos;s sake must be discouraged&amp;quot;
Of course we are joking h">
<meta property="og:type" content="website">
<meta property="og:title" content="Fully Convolutional Networks in Halide (Checkpoint)">
<meta property="og:url" content="http://yoursite.com/parallel-checkpoint/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Anbang Hu, Xingda Zhai
View us on Github
For project background information, please refer to our proposal webpage.
&amp;quot;Progress for progess&apos;s sake must be discouraged&amp;quot;
Of course we are joking h">
<meta property="og:image" content="https://raw.githubusercontent.com/xzhai1/latte/master/images/rgb.png">
<meta property="og:image" content="https://raw.githubusercontent.com/xzhai1/latte/master/outputs/result.png">
<meta property="og:updated_time" content="2016-04-20T00:39:30.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Fully Convolutional Networks in Halide (Checkpoint)">
<meta name="twitter:description" content="Anbang Hu, Xingda Zhai
View us on Github
For project background information, please refer to our proposal webpage.
&amp;quot;Progress for progess&apos;s sake must be discouraged&amp;quot;
Of course we are joking h">
<meta name="twitter:image" content="https://raw.githubusercontent.com/xzhai1/latte/master/images/rgb.png">
  
    <link rel="alternative" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css">
  

</head>
<body>
  <div id="container">
    <div id="wrap">
      <div class="outer">
        <section id="main"><article id="page-undefined" class="article article-type-page" itemscope itemprop="blogPost">
  
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Fully Convolutional Networks in Halide (Checkpoint)
    </h1>
  

      </header>
    
    <div class="article-meta">
      <a href="/parallel-checkpoint/index.html" class="article-date">
  <time datetime="2016-04-20T00:39:30.000Z" itemprop="datePublished">2016-04-19</time>
</a>
      
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p>Anbang Hu, Xingda Zhai</p>
<p><a href="https://github.com/xzhai1/latte" target="_blank" rel="external">View us on Github</a></p>
<p>For project background information, please refer to our <a href="http://haboric-hu.github.io/parallel-proposal/" target="_blank" rel="external">proposal webpage</a>.</p>
<h2>&quot;Progress for progess's sake must be discouraged&quot;</h2>
<p>Of course we are joking here. From 0 line of code to a fully fledged repo, from caffe/protobuf illiterate to reading and coding them at grade level, and from Halide admirers to practioners (or haters depending on perspectives), a lot can be said for our progress. Here are some of our milestones.</p>
<h3>ConvNets, Caffe and protobuf, a hell of a learning curve!</h3>
<p>We started the project by learning the mechanics of ConvNets. After all, we weren't ML/CV PhD's and we needed to start from 0. Stanford's CS231N class <a href="http://cs231n.github.io/" target="_blank" rel="external">notes</a> and <a href="http://cs231n.stanford.edu/syllabus.html" target="_blank" rel="external">website</a> were a godsend. They don't assume we have already published on the subjects, and do an excellent job in motivating and helping us visualize the network like we are 5-year-old's.</p>
<p>Then we went about getting acquainted with the framework that supposedly makes building and training such a network easy, so easy in fact that you don't even have to write a single line of code. One first finesses one's images into a format Caffe can read(LMDB or HDF5). Then one defines the desired network architecture and solver in a language Caffe speaks(protobuf, explained next). Lastly, if available, one supplies a pretrained model (if available or from previous training iterations) to Caffe to expedite the training/finetuning process. All is done without a line of code, that is if you stay <strong>within the framework</strong>.</p>
<p>However, since we are hacking the framework, we had to go out of the framework. To start, we had to deconstruct the trained model. An architecture with trained weights(collectively known as the model), like the <a href="https://github.com/shelhamer/fcn.berkeleyvision.org" target="_blank" rel="external">one</a> we are trying to construct, is saved in binary serialized by protobuf, a fantastic beast we have never set eyes let alone finding it.</p>
<p>Once we found it, it was indeed fantastic. Each compoment of Caffe, its layers, their parameters and the associated data are encapsulated in <a href="https://github.com/BVLC/caffe/blob/master/src/caffe/proto/caffe.proto" target="_blank" rel="external">caffe.proto</a>. They are human readable, not littered with accessor methods, and best of all, better documented than the Caffe website. Protobuf compiler generates <code>.h</code> and <code>.cpp</code> files containing accessor methods for all the data structure defined therein.</p>
<h3>Replicating layers</h3>
<p>On the component level, we have implemented the basic building blocks in Halide necessary to replicate the FCN. Summary as follows:</p>
<p>Vision Layer:
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Convolution</span><br><span class="line">Pooling</span><br><span class="line">Deconvolution</span><br></pre></td></tr></table></figure></p>
<p>Activation Layer:
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ReLU</span><br></pre></td></tr></table></figure></p>
<p>Common Layer:
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Crop</span><br><span class="line">Dropout</span><br><span class="line">Split</span><br><span class="line">Silence</span><br></pre></td></tr></table></figure></p>
<p>Loss Layer:
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">SoftmaxWithLoss</span><br><span class="line">Softmax</span><br></pre></td></tr></table></figure></p>
<p>On the network level, we created a class that can read in the trained model and build the network automatically. In addition, we wrote test harness to run our implementation.</p>
<h3>Scheduling in Halide</h3>
<p>So far, we experimented with Halide scheduling on CPU. <code>z</code> layers are independent of each other and therefore can be and should be processed in parallel. For each <code>z</code> layer, we also parallized across all tiles, and within each tile, vectorized all the operations.  As a preliminary result, after compiling the pipeline, and prior to deconvolution(a layer which we are still working on), a <code>300x300</code> image takes about 7 seconds to pass through the pipeline.</p>
<h2>Issues</h2>
<h3>Memory vs speed</h3>
<p>Currently, our pipeline in Halide is a bit slow. One facet of the lack of speed is the compilation time; when the pipeline is built, no function is yet realized. Before performing inference on an image, the whole pipeline needs to be compiled ''just in time'' which takes a significant amount of time. This compilation can't be conducted ahead of time because we don't know the image size ahead of the time.</p>
<p>A second facet of the slowness is we haven't truly explored the different scheduling possibilities. As described below, we don't have previliged access for the machines we need.</p>
<p>Currently, we are saving on memory. Each stage doesn't realize its function and pass around the intermediate images. Instead, Halide functions were passed around and the whole pipeline realized as the last stage. We conjectured that we are beaten by Caffe because we are not trading memory for speed as Caffe does in its implementation, e.g. <code>im2col</code> and <code>col2im</code> in Halide.</p>
<h3><code>sudo</code> make me a sandwich</h3>
<p>We started our development on our laptops and decided to move to a dedicated machine with real cores and GPUs as our testing was no longer constrained to single layer. We have encountered the following problem:</p>
<ol>
<li>On AFS machines, i.e. GHC and linux.andrew.cmu.edu, we have linking errors neither present on either LATEDAYS head node nor our laptops. We run Ubuntu 14.04.</li>
<li>On LATEDAYS head node, we have linking error relating to <code>libHalide.so</code> identical to the <a href="https://piazza.com/class/iirmvzoqbhk1hz?cid=1174" target="_blank" rel="external">issue</a> in Piazza. We tried building Halide from source with the LLVM fedora release, since LATEDAYS is RedHat Linux, with no success.</li>
</ol>
<p>We are hoping to resolve the issues with course staff soon but if we make no progress in the coming week, we will stick to our laptops.</p>
<h3><em>Caveat emptor</em></h3>
<p>Halide's programming mode is a &quot;gather&quot; model where we have to think about <code>x</code> and <code>y</code> as output positions and translate them into input positions. For example, in the case of convolution where one slides the kernel around the input image and reduce the input pixels under the kernel, in Halide, we have to think backwards: at each <code>x</code> and <code>y</code> position in the output, which one of the input indices are responsible for me? When one starts to yearn for <code>for</code> loop, one is not thinking in Halide's mindset. Let's do an example: convolution with stride of 2, implicit 0 padding.</p>
<p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">input </span><br><span class="line">---------------------------------</span><br><span class="line">| a | b | c | d | e | f | g | h |</span><br><span class="line">---------------------------------</span><br><span class="line">  0   1   2   3   4   5   6   7 </span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">kernel </span><br><span class="line">-------------</span><br><span class="line">| 1 | 2 | 3 |</span><br><span class="line">-------------</span><br><span class="line"></span><br><span class="line">output </span><br><span class="line">------------------------------------</span><br><span class="line">| 2b+3c | c+2d+3e | e+2f+3g | g+2h |</span><br><span class="line">------------------------------------</span><br><span class="line">    0        1         2       3</span><br></pre></td></tr></table></figure></p>
<p>Deconvolution, put it quite simply, is an upsampling step; it takes a small input and expand it to a large one. For the sake of clarity, we will use a 1D example to illustrate our point. Consider an input of size 4, and a kernel of 3 and an upsampling factor of 2, i.e. we want to expand the input by 2 times.</p>
<p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">input </span><br><span class="line">-----------------</span><br><span class="line">| a | b | c | d |</span><br><span class="line">-----------------</span><br><span class="line">  0   1   2   3</span><br><span class="line"> </span><br><span class="line">kernel </span><br><span class="line">-------------</span><br><span class="line">| 1 | 2 | 3 |</span><br><span class="line">-------------</span><br><span class="line"></span><br><span class="line">output </span><br><span class="line">---------------------------------</span><br><span class="line">|   |   |   |   |   |   |   |   |</span><br><span class="line">---------------------------------</span><br><span class="line">  0   1   2   3   4   5   6   7</span><br></pre></td></tr></table></figure></p>
<p>In our example, for each <code>x</code> position, the responsible input indices would be <code>[x/2 - 1, x/2, x/2 + 1]</code>. As a result, we will have the following picture:</p>
<p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">input </span><br><span class="line">-----------------</span><br><span class="line">| a | b | c | d |</span><br><span class="line">-----------------</span><br><span class="line">  0   1   2   3</span><br><span class="line"> </span><br><span class="line">kernel </span><br><span class="line">-------------</span><br><span class="line">| 1 | 2 | 3 |</span><br><span class="line">-------------</span><br><span class="line"></span><br><span class="line">output </span><br><span class="line">-----------------------------------------------------------------------</span><br><span class="line">| 2a+3b | 2a+3b | a+2b+3c | a+2b+3c | b+2c+3d | b+2c+3d | c+2d | c+2d |</span><br><span class="line">-----------------------------------------------------------------------</span><br><span class="line">    0      1        2         3          4         5       6      7</span><br></pre></td></tr></table></figure></p>
<p>More generally in the 1D case, consider a reduction domain, <code>r</code> of <code>[0, k]</code> where <code>k</code> is the kernel size and <code>f</code> be the upsampling factor, the output should be:</p>
<p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">output(x) = sum(kernel(r.x)*input(x/f + r.x - k/2 + 1)</span><br></pre></td></tr></table></figure></p>
<h2>Revised Plan</h2>
<p>In our original proposal, we stated:</p>
<blockquote>
<p>Our goal is to implement FCN using Halide and hopefully achieve faster training and testing without losing too much accuracy...</p>
</blockquote>
<blockquote>
<p>With these vision layers in Halide, we can combine them into different networks for evaluation. Our first step is to combine them into Halide version of fully convolutional AlexNet (Halide-FCN-AlexNet). We will finetune Halide-FCN-AlexNet and Caffe-FCN-AlexNet on a subset of PASCAL VOC (thousands of images) and compare the time needed for convergence. We will also test both nets to compare the time needed to do inference. Our expectation is that with Halide, we have more power in scheduling tasks, which can potentially increase the training speed and testing speed. Since the algorithm in FCN is the same as previous work [4], we also expect that the performance will not differ too much.</p>
</blockquote>
<blockquote>
<p>After testing Halide version of FCN-AlexNet, we will combine them into other networks and evaluate their performance against Caffe version.</p>
</blockquote>
<p>At current pace and with the difficulties we have faced, we have to decide to be more conservative. We will move the following two goals into the &quot;nice to have&quot; goals:</p>
<ol>
<li>&quot;hopefully achieve faster training&quot; -- we do not know if we can have backpropogation correctly implmented while trying to achieve comparable inference speed as caffe.</li>
<li>&quot;we will combine them into other networks and evaluate their performance against Caffe version.&quot; -- we decide to focus on the FCN version before trying to make our framework general. There is little point in creating all the layers out there if we can't tune and create comparable performance as caffe.</li>
</ol>
<h2>The Dog and Pony Show</h2>
<p>We plan to show a benchmark results for feedforward for both Caffe and our framework. Should we be able to solve our deployment issues on GHC or Latedays machines, we would also show the result for GPU trials. It would be nice to have backpropogation result if we have that completed. As we are still working on deconvolution, we can show the result of the stage before deconvolution for this input image.</p>
<p><img src="https://raw.githubusercontent.com/xzhai1/latte/master/images/rgb.png" alt="alt text"></p>
<p><img src="https://raw.githubusercontent.com/xzhai1/latte/master/outputs/result.png" alt="alt text"></p>
<h2>Schedule</h2>
<table>
<thead>
<tr>
<th>Week of</th>
<th></th>
<th>Task</th>
<th>Owner</th>
</tr>
</thead>
<tbody>
<tr>
<td>4/18 - 4/24</td>
<td>1st half</td>
<td>Figure out deconvolution implementation.</td>
<td>Haboric, X.D.</td>
</tr>
<tr>
<td></td>
<td>2st half</td>
<td>Install Caffe on a workable machine and produce benchmark feedforward result on the same model</td>
<td>X.D.</td>
</tr>
<tr>
<td>4/25 - 5/1</td>
<td>1st half</td>
<td>Explore Halide scheduling and performance tune</td>
<td>X.D.</td>
</tr>
<tr>
<td></td>
<td>2st half</td>
<td>(Re)implement convolution, deconvolution using im2col, col2im as well as efficient matrix matrix multiplication from fast parallel library.</td>
<td>Haboric</td>
</tr>
<tr>
<td>5/2 - 5/8</td>
<td>1st half</td>
<td>Explore possibility of backward propogation in Halide and benchmark whole pipeline against Caffe and performance tune</td>
<td>Haboric</td>
</tr>
<tr>
<td></td>
<td>2st half</td>
<td>Documentation, presentation preparation and report writeup</td>
<td>Haboric &amp; X.D.</td>
</tr>
</tbody>
</table>
<h2>Reference</h2>
<ol>
<li>
<p>Noh, Hyeonwoo, Seunghoon Hong, and Bohyung Han. &quot;Learning deconvolution network for semantic segmentation.&quot; Proceedings of the IEEE International Conference on Computer Vision. 2015.</p>
</li>
<li>
<p>http://www.vlfeat.org/matconvnet/</p>
</li>
<li>
<p>http://caffe.berkeleyvision.org/tutorial/layers.html</p>
</li>
<li>
<p>Long, Jonathan, Evan Shelhamer, and Trevor Darrell. &quot;Fully convolutional networks for semantic segmentation.&quot; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015.</p>
</li>
</ol>

      

      
        
    </div>
  </div>
  
    
  
</article>

</section>
        <aside id="sidebar">
  <nav class="menus">
  	<ul>
  		<li><a href="/"><i class="icon icon-home"></i></a></li>
  		
			<li><a href="/archives"><i class="icon icon-fenlei"></i></a></li>
  		
  		
			<li><a href="/tags"><i class="icon icon-tag"></i></a></li>
  		
  		
  			<li><a href="https://github.com/" target="_blank"><i class="icon icon-github"></i></a></li>
  		
  	</ul>
  </nav>
  <a id="go-top" href="#"><i class="icon icon-up"></i></a>
</aside>
      </div>
      <footer id="footer">
  
	<div id="footer-info" class="inner">
	  &copy; 2016 Anbang Hu 
	  - Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
	  - Theme <a href="https://github.com/Alex-fun/hexo-theme-jane/" target="_blank">Jane</a>
	</div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/tags" class="mobile-nav-link">Tag</a>
  
    <a href="https://github.com/" class="mobile-nav-link">Github</a>
  
</nav>
    

<script src="//cdn.bootcss.com/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>